
## 核心模型与能力

- **VLM (Vision-Language Model) / 视觉-语言模型**
    
    - **解释：** 一种能够同时处理和理解视觉信息（如图像、视频）和文本信息（如文字描述、问题）的人工智能模型，旨在实现跨模态的理解和交互。
        
- **SpatialVLM / 空间VLM** (本文提出的模型名称)
    
    - **解释：** 本文提出的一种特定的视觉-语言模型，旨在通过专门的训练数据和方法，增强其对空间关系的推理能力。
        
- **LLM (Large Language Model) / 大型语言模型**
    
    - **解释：** 经过大量文本数据训练的深度学习模型，能够理解和生成人类语言，并执行多种语言任务，如问答、文本摘要和机器翻译。
        
- **PaLM-E / PaLM-E** (特定模型名称)
    
    - **解释：** 一种具身（embodied）视觉-语言模型，能够理解和执行与物理世界交互的任务。
        
- **PaLM / PaLM** (特定模型名称)
    
    - **解释：** Google 开发的一种大型语言模型系列，以其强大的语言理解和生成能力著称。
        
- **PaLM 2-S / PaLM 2-S** (特定模型名称)
    
    - **解释：** PaLM 系列中一个较小的变体模型，通常在保持高性能的同时，降低了计算资源需求。
        
- **GPT-4V / GPT-4V** (特定模型名称)
    
    - **解释：** GPT-4 的一个多模态版本，支持视觉输入，并在许多视觉-语言任务上取得了最先进的性能。
        
- **LLaVA-1.5 / LLaVA-1.5** (特定模型名称)
    
    - **解释：** 一个开源的多模态大型语言模型，在视觉指令微调方面有出色表现。
        
- **InstructBLIP / InstructBLIP** (特定模型名称)
    
    - **解释：** 另一个开源的视觉-语言模型，专注于通过指令微调提升性能。
        
- **PaLI / PaLI** (特定模型名称)
    
    - **解释：** 一种编码器-解码器 VLM，在多语言语料库上训练，擅长图像描述和视觉问答。
        
- **PaLM 2-E / PaLM 2-E** (特定模型名称)
    
    - **解释：** PaLM-E 的更新版本，采用相同的训练流程但使用了更先进的 LLM 骨干网络，常作为研究生成数据效果的基线。
        

---

## 数据集与任务

- **VQA (Visual Question Answering) / 视觉问答**
    
    - **解释：** 一项人工智能任务，要求模型根据给定的图像和相关问题，生成准确的文字答案。
        
- **Spatial Reasoning Capabilities / 空间推理能力**
    
    - **解释：** 指模型理解和推断物体之间空间关系的能力，例如相对位置、距离、大小等。
        
- **Large-Scale Spatial Reasoning VQA Dataset / 大规模空间推理VQA数据集**
    
    - **解释：** 一种专为训练模型进行空间推理而设计的大型视觉问答数据集，包含大量关于物体空间关系的问答对。
        
- **Qualitative Questions / 定性问题**
    
    - **解释：** 询问对某种空间关系的判断性问题，答案通常是描述性的词语，而非具体的数值，例如“在左边”、“更高”等。
        
- **Quantitative Questions / 定量问题**
    
    - **解释：** 需要包含具体数字和单位的答案的问题，用于描述精确的空间属性，例如“距离多少米”、“宽度多少厘米”等。
        
- **Internet-scale Image-Captioning Datasets / 互联网规模的图像-字幕数据集**
    
    - **解释：** 包含大量从互联网收集的图像及其对应文字描述（字幕）的数据集，常用于训练视觉-语言模型。
        
- **Binary Predicates / 二元谓词**
    
    - **解释：：** 指的是描述两个实体之间关系的词或短语，例如“A 在 B 上面”、“A 比 B 大”等。在空间推理中，常用于表达两个对象间的空间关系判断。
        
- **WebLI Images / WebLI图像**
    
    - **解释：** 来源于 WebLI 数据集的图像，WebLI 是一个大规模的图像-文本对数据集，常用于预训练视觉-语言模型。
        
- **OKVQA Benchmark / OKVQA基准**
    
    - **解释：** 一个视觉问答基准数据集，主要关注需要外部常识知识来回答的问题。
        
- **VQA-v2 Test-dev Benchmark / VQA-v2 测试开发基准**
    
    - **解释：** VQA-v2 数据集的一个子集，用于模型评估和开发，包含需要模型理解图像内容并回答相应问题的数据。
        
- **Robotic Manipulation Dataset / 机器人操作数据集**
    
    - **解释：** 包含机器人执行操作任务时收集的数据，如抓取、放置物体等，通常包含精确的3D位置和深度信息。
        

---

## 空间推理方法与技术

- **Spatial Grounding / 空间基础**
    
    - **解释：** 指将语言中的空间概念（如“在...上面”、“在...旁边”）与图像中的实际视觉区域或对象建立对应关系的过程。
        
- **Chain-of-Thought Spatial Reasoning / 思维链空间推理**
    
    - **解释：** 一种推理范式，模型通过生成一系列中间推理步骤（“思维链”），逐步解决复杂的空间推理问题，模仿人类的逐步思考过程。
        
- **Template-based Approach / 基于模板的方法**
    
    - **解释：** 一种数据生成或问题构建的方法，通过预设的固定模式（模板）来生成多样化的内容，其中填充特定的变量或信息。
        
- **Object-centric Contexts Extraction / 以对象为中心的上下文提取**
    
    - **解释：** 从图像中识别并提取围绕特定对象的相关信息和特征，以便更好地理解该对象在场景中的作用和关系。
        
- **Basic Embodied Planning / 基本具身规划**
    
    - **解释：** 指模型能够为具身智能体（例如机器人）规划基本的动作序列，使其能够在物理环境中执行任务，涉及对物理空间和交互的理解。
        
- **Grounded Spatial Concept / 基础空间概念**
    
    - **解释：** 指将抽象的空间概念（如“高”、“近”）与图像中具体的视觉证据或量化数据联系起来，使其具有实际的物理意义。
        
- **Common Sense Knowledge / 常识知识**
    
    - **解释：** 人类在日常生活中通过经验积累的、普遍被接受和理解的知识，即使未被明确告知，也能用于推理。
        
- **Socratic Models / 苏格拉底模型** (特定模型范式)
    
    - **解释：** 一种通过结合多个专门模型（如视觉模型、语言模型）来解决复杂问题的方法，其中一个中心模型（通常是 LLM）负责协调和整合各个部分。
        
- **LLM as Coordinator / LLM作为协调者**
    
    - **解释：** 一种利用大型语言模型作为核心控制器或协调器，来调度和整合其他 AI 模型（如 VLM）以解决复杂任务的范式。
        
- **Chain-of-Thought Prompting / 思维链提示**
    
    - **解释：** 一种设计提示（prompt）的方法，引导大型语言模型生成一系列中间推理步骤（思维链），从而帮助其解决多步或复杂的推理任务。
        
- **Fine-grained Distance Estimation / 细粒度距离估计**
    
    - **解释：** 对物体之间或物体与相机之间距离进行精确、详细的数值估计。
        
- **Spatial Reasoning Common-sense / 空间推理常识**
    
    - **解释：** 模型在处理空间问题时展现出的，类似人类对空间关系和物理世界运作方式的直观理解。
        

---

## 计算机视觉模型与技术

- **Open-vocabulary Detection / 开放词汇检测**
    
    - **解释：** 一种目标检测技术，能够识别和定位图像中训练时未明确见过的类别物体，通过利用文本描述或语义信息进行泛化。
        
- **Metric Depth Estimation / 度量深度估计**
    
    - **解释：** 估计图像中每个像素到相机的实际物理距离（以米等单位表示），从而重建场景的三维结构。
        
- **Semantic Segmentation / 语义分割**
    
    - **解释：** 一种图像处理任务，将图像中的每个像素分类到预定义的语义类别中（例如，将所有属于“汽车”的像素标记为同一类别），从而实现像素级的场景理解。
        
- **Object-centric Captioning Models / 以对象为中心的图像描述模型 (或 物体中心图像描述模型)**
    
    - **解释：** 能够为图像中特定对象生成详细文字描述的 AI 模型，通常比为整张图片生成描述更聚焦、更细致。
        
- **CLIP-based Open-vocabulary Classification Model / 基于CLIP的开放词汇分类模型**
    
    - **解释：** 利用 CLIP（Contrastive Language–Image Pre-training）模型的能力，对图像进行分类，且能够识别未在训练中明确见过的类别，因为它通过文本描述进行匹配。
        
- **Region Proposal / 区域提案**
    
    - **解释：** 在目标检测任务中，算法生成图像中可能包含对象的候选区域（通常是边界框），以便后续的分类和精确定位。
        
- **Region Captioning / 区域图像描述**
    
    - **解释：** 为图像中特定区域或对象生成文字描述的技术，而不是为整张图片生成描述。
        
- **Pixel Clusters / 像素簇**
    
    - **解释：** 图像中具有相似特征或属于同一对象的相邻像素集合。
        
- **Open-vocabulary Caption Descriptions / 开放词汇图像描述**
    
    - **解释：** 能够使用广泛的词汇来描述图像内容，包括那些在训练数据中不常见的词汇。
        
- **2D Image Plane / 2D图像平面**
    
    - **解释：** 指的是图像的二维表面，只包含宽度和高度信息，不包含深度信息。
        
- **Depth Estimation / 深度估计**
    
    - **解释：** 从2D图像中推断场景中每个点的三维深度信息。
        
- **Monocular 2D Pixels / 单目2D像素**
    
    - **解释：** 仅由单个相机捕获的二维图像中的像素，缺乏直接的深度信息。
        
- **Metric-scale 3D Point Clouds / 度量尺度的3D点云**
    
    - **解释：** 由一系列具有 X、Y、Z 坐标的点组成的三维数据，每个点代表场景中的一个位置，并且这些坐标是基于实际物理单位（如米）的。
        
- **Canonicalize the Camera Coordinate System / 规范化相机坐标系**
    
    - **解释：** 将3D点云的坐标系从相机自身的局部坐标系转换到一个标准化的、统一的坐标系中，以便于不同图像之间的数据比较和整合。
        
- **Geodetic Coordinate System / 大地坐标系**
    
    - **解释：** 一种基于地球表面模型（如椭球体）的全球性三维坐标系统，常用于地理空间定位。
        
- **Horizontal Surface Segmentation / 水平表面分割**
    
    - **解释：** 识别并分割图像或点云中的水平平面区域，如地板、桌面等，这对于理解场景结构和进行坐标系转换很重要。
        
- **Frame Transfer / 帧转换**
    
    - **解释：** 将数据或坐标从一个参考框架（或坐标系）转换到另一个参考框架的过程。
        
- **Human-aligned Rounding Mechanism / 人类对齐的四舍五入机制**
    
    - **解释：** 一种在数字四舍五入时，使其结果更符合人类直觉和习惯的机制，而非严格的数学四舍五入。
        
- **2D Spatial Relationship Inference / 2D空间关系推理**
    
    - **解释：** 指模型在二维图像平面上推断物体之间空间关系的能力，例如左右、上下等。
        
- **3D Spatial Reasoning / 3D空间推理**
    
    - **解释：** 指模型在三维空间中推断物体之间空间关系的能力，涉及深度、高度、体积等三维概念。
        
- **SI Units / 国际单位制单位**
    
    - **解释：** 国际通用的测量单位系统，例如米（m）、千克（kg）、秒（s）等。
        
- **Visual Transformer (ViT) Encoder / 视觉 Transformer (ViT) 编码器**
    
    - **解释：** 一种基于 Transformer 架构的图像编码器，将图像分割成小块（patches）并将其作为序列输入，常用于图像分类和特征提取。
        
- **Frozen ViT / 冻结 ViT**
    
    - **解释：** 在模型训练过程中，ViT 编码器的参数被固定，不参与梯度更新，常用于利用预训练特征而不改变其底层表示。
        
- **Unfrozen ViT / 未冻结 ViT**
    
    - **解释：** 在模型训练过程中，ViT 编码器的参数允许被更新和微调，以便更好地适应特定任务的需求。
        
- **Ground Truth / 真值**
    
    - **解释：** 在机器学习和数据集中，指数据所对应的实际、正确的数值或标签，通常由人工标注或精确测量获得。
        
- **Expert Vision Models / 专家视觉模型**
    
    - **解释：** 经过特定训练、在某一视觉任务上表现出高精度的专业模型，例如用于深度估计或语义分割的模型。
        
- **Geometric Primitives / 几何原语**
    
    - **解释：** 构成复杂几何形状的基本几何元素，如点、线、平面、立方体、球体等。
        

---

## 数据处理与优化

- **Semantic Filtering / 语义过滤**
    
    - **解释：** 根据内容的语义意义对数据进行筛选，排除不符合特定要求或不适合某种任务的数据。
        
- **Ambiguity Resolution / 歧义消除**
    
    - **解释：** 解决文本或图像中可能存在的多种解释或指代不清的问题，确保每个实体或描述都有明确的含义。
        
- **FlexCap / FlexCap** (特定模型名称)
    
    - **解释：** 本文中提到的一种用户可配置的以对象为中心的图像描述方法，旨在生成更细粒度和无歧义的对象描述。
        
- **User-configurable Object-centric Captioning Approach / 用户可配置的以对象为中心的图像描述方法**
    
    - **解释：** 一种图像描述方法，允许用户自定义或调整其行为，以便更灵活地为图像中的特定对象生成描述。
        
- **Semantic-oriented Post-processing Algorithm / 面向语义的后处理算法**
    
    - **解释：** 在数据处理流程的后期，根据语义信息对结果进行调整、优化或修正的算法，以进一步提高准确性或消除歧义。
        
- **Visual Instruction Tuning / 视觉指令微调**
    
    - **解释：** 一种训练方法，通过在包含视觉和文本指令的数据集上微调模型，使其能够更好地理解并遵循视觉相关的指令。
        
- **Underfitting / 欠拟合**
    
    - **解释：** 机器学习模型的一种状态，指模型在训练数据上表现不佳，因为它未能充分学习数据的基本模式或关系。
        
- **Spatial VQA Supervisions / 空间 VQA 监督**
    
    - **解释：** 指在训练模型时，使用空间视觉问答数据来提供监督信号，以指导模型学习空间推理能力。
        
- **Noisy Quantitative Answers / 噪声定量答案**
    
    - **解释：** 指在数据集中，定量数值（如距离、尺寸）包含不精确或随机误差的答案。
        
- **Gaussian Noises / 高斯噪声**
    
    - **解释：** 一种常见的随机噪声类型，其数值分布服从高斯（正态）分布，常用于模拟数据中的随机误差。
        
- **Mean Squared Error (MSE) / 均方误差 (MSE)**
    
    - **解释：** 衡量预测值与真实值之间差异的常用指标，计算方式是误差平方的平均值，常用于回归任务中评估模型的准确性。
        
- **Bias towards the Mean / 偏向平均值**
    
    - **解释：** 模型预测结果倾向于靠近训练数据或真实数据的平均值，尤其是在高度正则化的情况下，这可能导致对极端值预测不准确。
        
- **Heavily Regularized / 重度正则化**
    
    - **解释：** 指在模型训练中施加了较强的正则化技术（如 L1/L2 正则化、Dropout 等），旨在防止模型过拟合，但有时也可能导致模型在某些细节上表现保守。
        

---

## 应用

- **Robotics / 机器人技术**
    
    - **解释：** 涉及设计、建造、操作和应用机器人的科学与工程领域。
        
- **Universal Open-vocabulary Reward Annotators / 通用开放词汇奖励标注器**
    
    - **解释：** 能够识别和量化各种任务（包括未预先定义的任务）的进展或成功，并生成奖励信号的系统。
        
- **Success Detector / 成功检测器**
    
    - **解释：：** 能够判断一个任务或动作是否成功完成的组件或模型。
        
- **Spatial Awareness / 空间感知**
    
    - **解释：** 模型或系统对物理空间、物体位置、方向和关系的理解能力。
        
- **Dense Reward Annotator / 密集奖励标注器**
    
    - **解释：** 在强化学习中，能够频繁地（例如，在每个时间步或轨迹的每个帧）提供奖励信号的系统，而不是只在任务结束时提供。
        
- **Spatial Reasoning Submodule / 空间推理子模块**
    
    - **解释：** 一个专门负责执行空间推理任务，并作为更大系统（如 LLM）一部分的模块。
        
- **Reward/Cost Function / 奖励/成本函数**
    
    - **解释：** 在强化学习中，用于量化智能体行为好坏的函数。奖励越高表示行为越好，成本越高表示行为越差。
        
- **Abate / 消融**
    
    - **解释：** 在机器学习实验中，指有选择地移除或禁用模型的某个组件、特征或训练条件，以评估其对整体性能的影响。
        

---